{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tạo máy ảo bằng Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill kernel cũ đi đồng thời tạo kết nối mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện cần thiết và chứng thực"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
    "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
    "!apt-get update -qq 2>&1 > /dev/null\n",
    "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cài đặt các thư viện liên quan đến việc kết nối Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phân tích cảm xúc với LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng ta sẽ dùng mạng LSTM để giải quyết bài toán phân tích cảm xúc (Sentiment Analysis) trên tập dữ liệu văn bản. Nếu nhìn theo kiểu black box, đầu vào của bài toán là một câu hoặc đoạn văn bản và đầu ra là trạng thái tích cực, tiêu cực hay trung hoà (positive - negative - neutral). Trong phạm vi của assignment này, chúng ta chỉ quan tâm đến hai trạng thái cảm xúc là positive và negative.\n",
    "\n",
    "![caption](Images/input_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Góc nhìn Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như chúng ta giữ nguyên định dạng đầu vào là chuỗi ký tự thì rất khó để thực hiện các thao tác biến đổi như tích vô hướng (dot product) hoặc các thuật toán trên mạng neural network như backpropagation. Thay vì dữ liệu đầu vào là một chuỗi, chúng ta cần chuyển đổi các từ trong tập từ điển sang dạng vector số học trong đó có thể thực hiện được các phép toán nêu trên.\n",
    "\n",
    "![caption](Images/word2vec.png)\n",
    "\n",
    "Trong hình minh hoạ ở trên, ta có thể hình dung dữ liệu đầu vào của thuật toán phân tích cảm xúc là một ma trận 16 x D chiều. Trong đó 16 là số lượng từ trong câu và D là số chiều của không gian vector để biểu diễn từ. Để ánh xạ từ một từ sang một vector, chúng ta sử dụng ma trận word embedding như đã thực hiện trong bài Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tập dữ liệu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment này, chúng tôi sử dụng tập dữ liệu review trên trang Foody với khoảng 30,000 mẫu được gán nhãn. Trong đó có 15,000 mẫu positive và 15,000 mẫu negative. Nguồn: https://streetcodevn.com/blog/dataset. Tập dữ liệu này đã được đính kèm trong thư mục của assignment 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các bước để huấn luyện trên mạng RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có 5 bước chính để giải quyết bài toán phân tích cảm xúc trong văn bản:\n",
    "\n",
    "    1) Huấn luyện một mô hình phát sinh ra vector từ (như mô hình Word2Vec) hoặc tải lên các vector từ tiền huấn luyện.\n",
    "    2) Tạo ma trận ID cho tập dữ liệu huấn luyện\n",
    "    3) Tạo mô hình RNN với các đơn vị LSTM, sử dụng tensorflow\n",
    "    4) Huấn luyện mô hình RNN với dữ liệu ma trận đã tạo ở bước 2\n",
    "    5) Đánh giá mô hình đã huấn luyện với tập test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load tập từ vựng và ma trận word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên, để có thể biến đổi một từ thành một vector, chúng ta sử dụng mô hình đã được huấn luyện trước đó (pretrained model). Mô hình đã train trước đó cho tiếng Việt được lấy ở đây: https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.vi.300.vec.gz\n",
    "\n",
    "Tuy nhiên, số lượng từ vựng tiếng Việt được huấn luyện rất lớn, khoảng 2M từ. Mỗi từ được biểu diễn dưới dạng một vector 300 chiều. Với kích thước gốc của ma trận word embedding như vậy sẽ gây khó khăn cho việc load dữ liệu cũng như đưa vào thư viện tensorflow để huấn luyện nên chúng tôi đã tối giản lại với số lượng từ tối thiểu để có thể chạy được trên tập dữ liệu review về đồ ăn của Foody.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified vocabulary loaded!\n",
      "Word embedding matrix loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# LƯU Ý: CẦN PHẢI CHỈNH LẠI ĐƯỜNG DẪN NÀY THÀNH THƯ MỤC CHỨA CÁC FILE ASSIGNMENT3\n",
    "# CHỮ 'drive' có nghĩa là thư mục mặc định của Google drive\n",
    "currentDir = 'Assignment3-SentimentAnalysis-with-LSTM'\n",
    "\n",
    "wordsList = np.load(os.path.join(currentDir, 'wordsList.npy'))\n",
    "#wordsList = np.load('wordslist.npy')\n",
    "print('Simplified vocabulary loaded!')\n",
    "wordsList = wordsList.tolist()\n",
    "#wordsList = [word.decode('UTF-8') for word in wordsList] #Encode words as UTF-8\n",
    "wordVectors = np.load(os.path.join(currentDir, 'wordVectors.npy'))\n",
    "#wordVectors = np.load('wordVectors.npy')\n",
    "wordVectors = np.float32(wordVectors)\n",
    "print ('Word embedding matrix loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để chắc chắn mọi dữ liệu được load lên một cách chính xác, chúng ta cần kiểm tra xem số lượng từ trong từ điển rút gọn và số chiều của ma trận word embedding có khớp với nhau hay không? Trong trường hợp này số từ mà chúng tôi giữ lại là 19,899 và số chiều trong không gian biểu diễn là 300 chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary:  19899\n",
      "Size of the word embedding matrix:  (19899, 300)\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary: ', len(wordsList))\n",
    "print('Size of the word embedding matrix: ', wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec trên một từ đơn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có thể xác định được vector biểu diễn của một từ tiếng Việt. Đầu tiên chúng ta sẽ xác định xem vị trí của từ đó trong wordsList. Sau đó lấy vector ở dòng tương ứng trên trên ma trận wordVectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of `ngon` in wordsList:  14598\n",
      "Vector representation of `ngon` is:  [-2.040e-02 -9.800e-03  2.290e-01 -3.770e-02  5.430e-02 -2.680e-02\n",
      "  2.190e-02 -6.290e-02 -2.200e-02 -1.010e-02  8.300e-03 -8.810e-02\n",
      " -3.630e-02  7.820e-02 -7.780e-02 -4.930e-02 -6.600e-03 -1.026e-01\n",
      " -1.040e-02  5.380e-02  4.100e-02  6.530e-02 -2.770e-02 -6.340e-02\n",
      "  2.270e-02  4.420e-02  3.340e-02 -4.960e-02  8.290e-02 -3.990e-02\n",
      "  3.750e-02  1.800e-02 -1.115e-01 -7.200e-02 -5.060e-02 -1.051e-01\n",
      " -4.560e-02 -1.765e-01 -3.300e-02 -6.800e-03  5.580e-02 -4.180e-02\n",
      "  4.380e-02  4.940e-02  7.400e-03  4.020e-02 -8.850e-02 -9.840e-02\n",
      " -5.210e-02 -5.500e-03  3.730e-02 -8.460e-02 -6.910e-02 -4.980e-02\n",
      " -3.910e-02 -4.980e-02 -8.690e-02  6.100e-03 -5.360e-02 -3.800e-03\n",
      "  1.162e-01 -4.160e-02  5.000e-03 -7.240e-02 -3.320e-02  1.800e-02\n",
      "  1.200e-02 -4.420e-02  1.350e-01  6.580e-02 -1.110e-02  1.960e-02\n",
      "  1.750e-02  2.010e-02  2.200e-03  1.810e-01 -6.610e-02 -6.860e-02\n",
      " -4.690e-02  7.890e-02  6.880e-02 -5.320e-02  2.770e-02  5.710e-02\n",
      " -1.183e-01  4.170e-02 -8.200e-02 -5.900e-02  8.790e-02  9.640e-02\n",
      "  6.000e-02  1.330e-02 -3.640e-02 -1.110e-02 -2.200e-02  1.770e-02\n",
      " -3.420e-02 -4.020e-02  3.590e-02  1.467e-01 -1.730e-02 -2.650e-02\n",
      "  6.400e-02  7.000e-03 -3.930e-02 -5.540e-02 -4.360e-02  8.000e-02\n",
      " -5.480e-02  3.840e-02 -8.330e-02  7.070e-02 -9.100e-03  2.480e-02\n",
      " -7.500e-03  3.030e-02 -6.600e-03 -9.800e-03  7.640e-02 -9.300e-03\n",
      "  2.330e-02 -2.000e-02  5.970e-02 -2.680e-02 -2.000e-02 -9.700e-03\n",
      " -5.310e-02 -9.820e-02 -2.570e-02  2.400e-02 -5.860e-02  1.820e-02\n",
      " -4.280e-02  9.580e-02  3.400e-02 -7.100e-03 -6.200e-03  1.239e-01\n",
      "  4.830e-02 -4.050e-02  4.810e-02 -1.093e-01  1.540e-02 -3.860e-02\n",
      "  1.250e-01 -7.950e-02  6.800e-03  7.420e-02  5.500e-02 -4.130e-02\n",
      " -2.090e-02 -2.250e-02  3.960e-02 -1.086e-01 -2.200e-02 -4.420e-02\n",
      "  1.965e-01 -2.260e-02  1.196e-01  1.200e-02  1.199e-01 -8.700e-03\n",
      "  4.260e-02  3.460e-02 -3.780e-02  1.951e-01 -9.300e-03 -6.260e-02\n",
      "  2.730e-02  7.340e-02  1.800e-03  5.080e-02 -3.470e-02 -9.680e-02\n",
      " -1.278e-01  3.790e-02  6.920e-02 -5.300e-02 -1.020e-01  1.076e-01\n",
      "  1.361e-01 -7.390e-02  9.650e-02 -2.250e-02  1.597e-01 -2.750e-02\n",
      " -4.200e-03  1.032e-01 -4.910e-02 -7.100e-03 -1.840e-02  7.240e-02\n",
      " -2.040e-02 -5.010e-02 -2.000e-04 -4.190e-02 -5.720e-02 -8.000e-03\n",
      "  9.780e-02 -7.130e-02 -6.070e-02 -1.740e-02 -1.290e-02  8.250e-02\n",
      "  6.600e-03 -2.250e-02 -5.100e-02  6.520e-02 -1.870e-02  5.790e-02\n",
      "  1.814e-01 -1.220e-01  4.770e-02  5.300e-02 -4.230e-02  2.139e-01\n",
      " -9.100e-03  1.314e-01 -3.600e-02 -3.780e-02  4.260e-02  3.000e-04\n",
      " -8.200e-02  1.570e-02 -1.380e-02  3.420e-02 -2.080e-02  1.790e-01\n",
      "  5.240e-02 -1.464e-01  6.330e-02  5.620e-02  2.000e-03 -6.490e-02\n",
      "  4.000e-04 -1.310e-02  1.020e-02  6.380e-02 -1.190e-02  2.440e-02\n",
      " -1.430e-02  1.027e-01  3.200e-03 -1.120e-02  8.270e-02  5.690e-02\n",
      "  2.740e-02 -9.800e-02 -3.150e-02 -9.750e-02 -1.660e-02  7.640e-02\n",
      " -4.960e-02 -7.940e-02  1.177e-01 -2.800e-03  6.860e-02 -5.930e-02\n",
      "  7.470e-02  5.790e-02  3.450e-02  5.550e-02 -3.380e-02  1.292e-01\n",
      "  3.840e-02  7.440e-02 -6.450e-02  2.470e-02 -1.810e-02  9.840e-02\n",
      " -1.329e-01 -6.380e-02 -8.360e-02 -3.580e-02  6.500e-03  8.240e-02\n",
      " -6.140e-02 -1.116e-01  2.310e-02  8.070e-02 -1.670e-02  4.150e-02\n",
      " -8.210e-02  6.290e-02 -5.580e-02  2.600e-03 -2.170e-02  3.200e-03\n",
      " -5.500e-03  6.040e-02  2.990e-02 -1.061e-01  5.200e-02  7.560e-02\n",
      "  6.250e-02  1.007e-01 -1.080e-01 -5.420e-02 -6.620e-02  6.080e-02]\n"
     ]
    }
   ],
   "source": [
    "ngon_idx = wordsList.index('ngon')\n",
    "print('Index of `ngon` in wordsList: ', ngon_idx)\n",
    "ngon_vec = wordVectors[ngon_idx]\n",
    "print('Vector representation of `ngon` is: ', ngon_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.1: Word2Vec để biểu diễn một đoạn văn bản"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nâng cấp hơn so với phiên bản Word2Vec cho từ đơn, phần này chúng ta sẽ biểu diễn một câu dưới dạng một ma trận gồm các vector biểu diễn của từng từ chồng lên nhau.\n",
    "\n",
    "Ví dụ như chúng ta muốn biểu diễn câu \"Món này ăn hoài không biết chán\". Đầu tiên, với mỗi từ trong câu ta sẽ tìm chỉ số tương ứng trong từ điển và lưu vào vector đặt tên là 'sentenceIndexes'. Sau đó, chúng ta có thể sử dụng hàm tra cứu ma trận word embedding của thư viện Tensorflow tf.nn.embedding_lookup để tra các vector tại các chỉ số trong 'sentenceIndexes'. Như vậy nếu chúng ta sử dụng tối đa 10 từ để lưu trữ cho một câu thì ma trận biểu diễn cho câu sẽ là một ma trận kích thước 10 x 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!activate tf-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "Row index for each word:  [  119  8136  4884 18791 16614 15951  3371     0     0     0]\n",
      "Sentence representation of word vectors:\n",
      "(10, 300)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "maxSeqLength = 10   #Maximum length of sentence\n",
    "numDimensions = 300 #Dimensions for each word vector\n",
    "sentenceIndexes = np.zeros((maxSeqLength), dtype='int32')\n",
    "\n",
    "# TODO 3.1: Gán chỉ số của các từ trong câu và 'sentenceIndexes'\n",
    "sentenceIndexes[0] = wordsList.index('món')\n",
    "sentenceIndexes[1] = wordsList.index('này')\n",
    "sentenceIndexes[2] = wordsList.index('ăn')\n",
    "sentenceIndexes[3] = wordsList.index('hoài')\n",
    "sentenceIndexes[4] = wordsList.index('không')\n",
    "sentenceIndexes[5] = wordsList.index('biết')\n",
    "sentenceIndexes[6] = wordsList.index('chán')\n",
    "\n",
    "# Các chỉ số 7, 8, 9 của sentenceIndexes  vẫn được gán bằng 0 như cũ\n",
    "print(sentenceIndexes.shape)\n",
    "print('Row index for each word: ', sentenceIndexes)\n",
    "\n",
    "# Ma trận biểu diễn:\n",
    "print('Sentence representation of word vectors:')\n",
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors,sentenceIndexes).eval().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như thực hiện đúng thì vector 'sentenceIndexes' sẽ có giá trị là: [119, 8136, 4884, 18791, 16614, 15951, 3371, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Khảo sát tập dữ liệu huấn luyện và tạo ma trận ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong assignment 3, chúng tôi sử dụng tập dữ liệu lấy từ trang web Foody trên miền dữ liệu liên quan đến ẩm thực. Tập dữ liệu bao gôm 15.000 review tích cực đặt trong thư mục 'positiveReviews' và 15.000 review tiêu cực đặt trong thư mục 'negativeReviews'. Do khối lượng dữ liệu lớn, nếu chúng ta chọn số lượng từ tối đa (maxSeqLength) quá cao thì sẽ bị lãng phí khi biểu diễn ở những câu review quá ngắn. Ngược lại, nếu sử dụng số lượng từ tối đa quá ít thì sẽ bị bỏ lỡ những từ quan trọng giúp cho việc phân tích cảm xúc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 30000\n",
      "The total number of words in the files is 1770824\n",
      "The average number of words in the files is 59.02746666666667\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "positiveFiles = ['positiveReviews/' + f for f in listdir('positiveReviews/') if isfile(join('positiveReviews/', f))]\n",
    "negativeFiles = ['negativeReviews/' + f for f in listdir('negativeReviews/') if isfile(join('negativeReviews/', f))]\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể sử dụng thư viện Matplot để minh hoạ phân bố về chiều dài của các câu review trong tập dữ liệu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axis([0, 1200, 0, 8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dựa trên biểu đồ histogram ở trên chúng ta có thể thấy là 180 là kết quả tương đối hợp lý. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxSeqLength = 180"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có cảm nhận rõ hơn về dữ liệu, chúng ta có thể hiển thị một số review bất kỳ như sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A positive sentence: \n",
      "Hôm_nay mình quay lại quán cùng_với mấy người bạn ở Sài_Gòn xuống , cũng lâu lắm rồi mình không ghé quán ốc này luôn nên được bữa ăn cho đã nè Vẫn không_gian tấp_nập xô_bồ , nhưng hên là vẫn còn bàn cho nhóm tụi mình , nhưng đợi xếp bàn hơi lâu không được vô rồi ngồi liền đâu , hôm_nay lại là ngày cao_điểm nữa\n",
      "\n",
      "Đông như_thế_này thì đồng_nghĩa với việc đợi thức_ăn dài cổ rồi nhưng không nha chỉ cần gọi 1 xíu thôi là có liền à , vì có mấy cô nấu lận nên nhanh cực_kì , quán này toàn hải_sản tươi_sống mỗi ngày thôi , bạn_bè mình ăn khen tới_tấp nên tụi nó ăn nhanh quá không chụp được mấy món kia\n",
      "\n",
      "Ấn_tượng cái món ốc giác á , đó giờ không dám ăn vì hơi sợ nhưng thằng bạn mình gọi món này nhìn cũng hấp_dẫn , ốc giác được chế_biến sạch_sẽ , trộn với xoài , đặc_biệt nhiều ớt để khử mùi tanh ấy nên ăn_không bị ớn\n",
      "\n",
      "Còn mì và ốc mỡ là ok quá rồi , chỗ này làm ngon nhất mà ăn cũng rất là no luôn\n",
      "\n",
      "Mấy cô phục_vụ vẫn thế , vẫn lạnh_lùng nhưng cô tận_tình với khách quá việc quán đông mà vẫn đứng đợi tụi mình order dù quán đang đông\n",
      "\n",
      "A negative sentence: \n",
      "- Điểm trừ : Mọi thứ đều được \" xắt lát mỏng \" , rau_muống thì \" dai_nhách \" , cải thảo thì bị úng : 3\n",
      "\n",
      "- Điểm cộng : Nước lẩu thì vừa ăn , cũng khá ngon . : )\n",
      "\n",
      "- Vì là phong_cách tự_phục_vụ nên không có comment gì được\n",
      "\n",
      "= > Lời khuyên : Chỉ nên ăn \" lót_dạ \" cho_biết thui , chớ ăn nhiu đây đói lắm à : 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('A positive sentence: ')\n",
    "fname = positiveFiles[3] # Randomly select a positive file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "\n",
    "print('A negative sentence: ')\n",
    "fname = negativeFiles[10] # Randomly select a negative file to view\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn hoá văn bản và tách từ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để tiết kiệm công sức và cũng nằm ngoài phạm vi của khoá học, chúng tôi đã chuẩn bị sẵn tập dữ liệu đã được tách từ. Giữa hai từ có thể ghép lại để tạo thành một khái niệm mới chúng tôi sử dụng ký tự '_' để nối các từ đó. Ví dụ: 'sinh_viên', 'sinh_học'.\n",
    "\n",
    "Chúng tôi chuẩn bị sẵn các hàm chuẩn hoá văn bản nhằm loại bỏ các ký tự đặc biệt. Tham khảo ở hàm 'cleanSentences'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes punctuation, parentheses, question marks, etc., and leaves only alphanumeric characters\n",
    "import re\n",
    "strip_special_chars = re.compile(\"[^\\w0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ chúng ta sẽ biểu diễn 30.000 review dưới dạng các chỉ số của các từ. Tập dữ liệu positive và negative sẽ được tính hợp lại thành một ma trận 30000x180. Trong đó 30000 là số lượng review và 180 là số lượng từ tối đa cho một câu. Do bước chuẩn bị này tốn khá nhiều tài nguyên tính toán nên sau khi tính toán xong, chúng ta sẽ lưu lại để sử dụng cho những lần chạy thí nghiệm sau. Ma trận lưu trữ các chỉ số này là: 'ids'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.2: xác định chỉ số của từng từ trong review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong phần này chúng ta sẽ tiến hành tra cứu từng từ trong review, sau đó gán vào ma trận 'ids'. Trong đó chỉ số dòng của ma trận tương ứng với file review, chỉ số cột của ma trận tương ứng với một từ của review. Trường hợp từ nào không có trong tập từ điển thì ta sẽ gán bằng chỉ số của từ 'UNK' (unknow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive files are indexed!\n",
      "Negative files are indexed!\n"
     ]
    }
   ],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "nFiles = 0\n",
    "# Index of Unknow word\n",
    "unk_idx = wordsList.index('UNK')\n",
    "\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            # TODO 3.2: Nếu 'word' thuộc tập 'wordsList' thì gán chỉ số của 'word' vào ma trận ids\n",
    "            try:\n",
    "                ids[nFiles][nIndexes] = wordsList.index(word)\n",
    "            # Ngược lại: gán 'unk_idx' vào ma trận ids\n",
    "            except:\n",
    "                ids[nFiles][nIndexes] = unk_idx\n",
    "            \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "\n",
    "print('Positive files are indexed!')\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding=\"utf-8\") as f:\n",
    "        nIndexes = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            # ToDo 3.2: tương tự như trên. Không khác gì hết.\n",
    "            try:\n",
    "                ids[nFiles][nIndexes] = wordsList.index(word)\n",
    "            except:\n",
    "                ids[nFiles][nIndexes] = unk_idx\n",
    "                \n",
    "            nIndexes = nIndexes + 1\n",
    "            if nIndexes >= maxSeqLength:\n",
    "                break\n",
    "        nFiles = nFiles + 1 \n",
    "\n",
    "print('Negative files are indexed!')\n",
    "# Save ids Matrix for future uses.\n",
    "#np.save(os.path.join(currentDir,'idsMatrix.npy'), ids)\n",
    "np.save('idsMatrix.npy',ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indexes of the first review:  [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821\n",
      " 14794  8884  6443  5767  8589 18850 15570  5596   799 11060  4222 16893\n",
      " 13078  8136  3364  4454  4756 10304  8885  3553  9782  1232 14359 10606\n",
      "   579 15522  2219 15092 14855 15253  4884  3364  5519  4558  9649   269\n",
      " 15522 12309 14855 11503  2212  4884  7155 11577  4222  5767 15076 12225\n",
      " 10774  1218  2876 19584  4558  2974 13452  5013   842 10642 17292 11895\n",
      "   803 11060 16760  1906 15253 14598 15253  1047  5668  4884 10642 12225\n",
      "  7090 17292 18109 13078 16334  1238  3364  5519  4135  3553 14967  4964\n",
      " 15385  9673  2997 14855  7446  8038 11440  1345   842  5767   803 11060\n",
      " 18791  5013     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# LƯU Ý: Bước thực hiện trên tương đối mất thời gian.\n",
    "# Trường hợp đã tính toán và lưu ma trận 'ids' rồi thì ta có thể load lên để sử dụng luôn\n",
    "ids = np.load(os.path.join(currentDir,'idsMatrix.npy'))\n",
    "#ids = np.load('idsMatrix.npy')\n",
    "print('Word indexes of the first review: ', ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu như quá trình chuyển từ câu dạng văn bảng sang vector các chỉ số trong từ điển ở trên đúng thì ids[0] sẽ nhận giá trị: [19898  1906  4454  5284 10661 11694 11994 18784 18569 18619 13174  9821 ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng hàm lấy dữ liệu train và test theo từng batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dưới đây chúng tôi xây dựng các hàm để lấy dữ liệu train và test theo từng batch. Bạn hãy giải thích tại sao lại có các con số 13999, 14999, 15999, 29999 nhé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            # Pick positive samples randomly\n",
    "            num = randint(1,13999)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            # Pick negative samples randomly\n",
    "            num = randint(15999,29999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(13999,15999)\n",
    "        if (num <= 14999):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Xây dựng RNN Model với Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên chúng tôi sẽ khởi tạo các tham số cho mô hình mạng RNN với các cell là các LSTM. Kiến trúc mạng ở đây bao gồm 128 đơn vị cho mỗi lớp, số lượng layer là 2, số lượng phân lớp là 2 và số vòng lặp khi huấn luyện là 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize paramters\n",
    "numDimensions = 300\n",
    "batchSize = 64\n",
    "lstmUnits = 128\n",
    "nLayers = 2\n",
    "numClasses = 2\n",
    "iterations = 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để lưu trữ dữ liệu input và ouput, chúng ta sẽ sử dụng hai kiểu dữ liệu placeholder. Một trong những điều quan trọng nhất khi khởi tạo các biến input và output này là xác định kích thước của các tensor. Mỗi output của mạng (hay còn gọi là label) sẽ là một vector one hot với hai giá trị tương ứng với hai loại cảm xúc: [1, 0] cho positive và [0, 1] cho negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/data_batch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.3: Xác định input và output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khởi tạo hai biến 'inputs' và 'labels' bằng kiểu placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# TODO 3.3: Khởi tạo hai biến 'inputs' và 'labels'\n",
    "inputs = tf.placeholder(tf.int32, [batchSize,maxSeqLength])\n",
    "labels = tf.placeholder(tf.float32,[batchSize,numClasses])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó tạo dữ liệu word vector từ khối dữ liệu đầu vào với ma trận word embedding. Nếu như quá trình khởi tạo đúng thì sẽ tạo ra các kiểu dữ liệu sau:\n",
    "labels --> Tensor(\"Placeholder:0\", shape=(64, 2), dtype=float32)\n",
    "inputs --> Tensor(\"Placeholder_1:0\", shape=(64, 10), dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/embedding_data.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.nn.embedding_lookup(wordVectors, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy sau bước này chúng ta đã có dữ liệu để đưa vào mạng mạng các LSTM. Để khởi tạo một LSTM chúng ta sử dụng hàm tf.nn.rnn_cell.BasicLSTMCell. Hàm này cần tham số đầu vào là số lượng đơn vị muốn khởi tạo. Đây chính là một hyperparamter đã được khởi tạo trước đó.\n",
    "Để chống lại việc overfitting, chúng ta sử dụng lớp dropout. \n",
    "\n",
    "Để tăng tính phức tạp cho kiến trúc mạng chúng ta chồng các lớp LSTM lên nhau (Stack LSTM Layers). Trong trường hợp này chúng ta sử dụng 2 lớp LSTM. Việc chồng thêm các lớp LSTM sẽ giúp cho mô hình có khả năng nhớ nhiều thông tin hơn nhưng đồng thời cũng làm tăng số lượng tham số khi huấn luyện. Điều này cũng có nghĩa là sẽ làm tăng thời gian huấn luyện cũng như là cần thêm nhiều dữ liệu hơn.\n",
    "\n",
    "Cuối cùng là đưa toàn bộ dữ liệu đầu vào vào mạng các LSTM sử dụng hàm tf.nn.dynamic_rnn. Chi tiết kiến trúc mạng LSTM sử dụng cho bài tập này được mô tả trong hình sau:\n",
    "\n",
    "![caption](Images/architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-d1fc5fa17842>:3: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "Tensor(\"rnn/transpose_1:0\", shape=(64, 180, 128), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def generate_a_lstm_layer():\n",
    "    # Khởi tạo một LSTM layer với 'lstmUnits' unit sử dụng hàm tf.contrib.rnn.BasicLSTMCell\n",
    "    lstmLayer = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "    # Sau đó tạo một lớp dropout để chống overfitting với hệ số out_keep_prob bằng 0.75\n",
    "    # Sử dụng hàm tf.contrib.rnn.DropoutWrapper\n",
    "    lstmLayer = tf.contrib.rnn.DropoutWrapper(cell=lstmLayer, output_keep_prob=0.75)\n",
    "    return lstmLayer\n",
    "\n",
    "# Sau khi đã có hàm tạo một LSTM Layer, ta sử dụng hàm này để chồng các LSTM lên\n",
    "# Stack các LSTM layer với hàm tf.nn.rnn_cell.MultiRNNCell\n",
    "lstmLayers = tf.nn.rnn_cell.MultiRNNCell([generate_a_lstm_layer() for i in range(nLayers)], state_is_tuple=True)\n",
    "# Feed data variable vào mạng LSTM sử dụng hàm tf.nn.dynamic_rnn\n",
    "initial_state = lstmLayers.zero_state(batchSize, tf.float32)\n",
    "outputs, _ = tf.nn.dynamic_rnn(lstmLayers,data,dtype=tf.float32,initial_state=initial_state)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi ra khỏi mạng LSTM, biến outputs sẽ là một tensor có kích thước [batchSize x maxSeqLength x lstmUnits], cụ thể là [64 x 180 x 128]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, chúng ta chỉ lấy dữ liệu ở LSTM cell cuối cùng và cho đi qua lớp kết nối đầy đủ để phân loại thành 2 trạng thái. Chỉ số của LSTM cell cuối cùng là 179 (do có 180 cell theo chiều ngang)  nên để có thể lấy được giá trị ta sẽ chuyển vị về tensor có kích thước [maxSeqLength x batchSize x lstmUnits] hay [180 x 64 x 128]. Sử dụng hàm tf.gather để lấy tensor thứ 179 có kích thước [64 x 128] bao gồm 64 mẫu vector 128 chiều. Vector 128 chiều này sẽ được đưa vào lớp fully connected để chuyển đổi về vector 2 chiều tương ứng với 2 trạng thái.\n",
    "\n",
    "Lớp kết nối đầy đủ bao gồm các bộ tham số 'weight' và 'bias' để thực hiện việc dự đoán kết quả. Bước này chính là tạo một lớp Fully Connected như trong sơ đồ kiến trúc mạng LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "\n",
    "# Lấy giá trị output tại LSTM cell cuối cùng\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "last = tf.gather(outputs, int(outputs.get_shape()[2]) - 1)\n",
    "# Đưa qua mạng Fully Connected mà không có activation function\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để xác định độ chính xác của hệ thống, ta đếm số lượng labels khớp với giá trị dự đoán (prediction). Sau đó tính độ chính xác bằng cách tính giá trị trung bình của các kết quả trả về đúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctResult = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctResult, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó chúng ta sẽ xác định hàm độ lỗi sử dụng softmax cross entropy được tính từ dữ liệu dự đoán và tập labels. Cuối cùng là chọn thuật toán tối ưu với tham số learning rate mặc định là 0.001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-20-773132a4e1b5>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng Tensorboard để visualize kết quả"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong quá trình huấn luyện, chương trình sẽ ghi log về độ lỗi và độ chính xác trên tập train vào thư mục 'tensorboard', lưu lại model sau mỗi 2000 vòng lặp ở thư mục 'models'. Việc huấn luyện trên 30,000 vòng lặp mất khoảng vài tiếng với GPU K80 được cung cấp bởi Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Huấn luyện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với mỗi vòng lặp, ta sẽ lấy ra một batch dữ liệu train để đưa vào mạng sử dụng `feed_dict`. với các tham số input và label là các placeholders. Bước huấn luyện này được lặp lại cho đến khi hết số lần cần huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6527f9b8df51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mnextBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnextBatchLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTrainBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Feed to optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnextBatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnextBatchLabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#Write summary to Tensorboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    # TODO 3.5\n",
    "    # Get next training batch\n",
    "    nextBatch, nextBatchLabels = getTrainBatch()\n",
    "    # Feed to optimizer\n",
    "    sess.run(optimizer, {inputs:nextBatch,labels:nextBatchLabels})\n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {inputs: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "\n",
    "    # Save model every 2000 training iterations\n",
    "    if (i % 2000 == 0 and i != 0):\n",
    "        #save_path = saver.save(sess, os.path.join(currentDir,\"models/pretrained_lstm.ckpt\"), global_step=i)\n",
    "        save_path = saver.save(sess,'models/pretrained_lstm.ckpt',global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load mô hình đã train và đánh giá mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thời gian huấn luyện mạng khá lâu, nên trong quá trình mạng đang được huấn luyện, ta sẽ lưu lại một số checkpoint. Để có thể test thử trên một checkpoint mới nhất ta sử dụng hàm tf.train.latest_checkpoint và truyền vào tên thư mục muốn lấy model mới nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models\\pretrained_lstm.ckpt-28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam not found in checkpoint\n\t [[node save_2/RestoreV2 (defined at <ipython-input-24-360790a6aa8a>:2)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_2/RestoreV2', defined at:\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-360790a6aa8a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1102, in __init__\n    self.build()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 795, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 862, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam not found in checkpoint\n\t [[node save_2/RestoreV2 (defined at <ipython-input-24-360790a6aa8a>:2)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Variable/Adam not found in checkpoint\n\t [[{{node save_2/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         sess.run(self.saver_def.restore_op_name,\n\u001b[1;32m-> 1546\u001b[1;33m                  {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[0;32m   1547\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key Variable/Adam not found in checkpoint\n\t [[node save_2/RestoreV2 (defined at <ipython-input-24-360790a6aa8a>:2)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_2/RestoreV2', defined at:\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-360790a6aa8a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1102, in __init__\n    self.build()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 795, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 862, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Key Variable/Adam not found in checkpoint\n\t [[node save_2/RestoreV2 (defined at <ipython-input-24-360790a6aa8a>:2)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1555\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1556\u001b[1;33m         \u001b[0mnames_to_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_graph_key_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1557\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mobject_graph_key_mapping\u001b[1;34m(checkpoint_path)\u001b[0m\n\u001b[0;32m   1829\u001b[0m   object_graph_string = reader.get_tensor(\n\u001b[1;32m-> 1830\u001b[1;33m       checkpointable.OBJECT_GRAPH_PROTO_KEY)\n\u001b[0m\u001b[0;32m   1831\u001b[0m   object_graph_proto = (\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mget_tensor\u001b[1;34m(self, tensor_str)\u001b[0m\n\u001b[0;32m    370\u001b[0m         return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str),\n\u001b[1;32m--> 371\u001b[1;33m                                           status)\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Key _CHECKPOINTABLE_OBJECT_GRAPH not found in checkpoint",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-360790a6aa8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#saver.restore(sess, tf.train.latest_checkpoint(os.path.join(currentDir,'models')))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1560\u001b[0m         \u001b[1;31m# a helpful message (b/110263146)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m         raise _wrap_restore_error_with_msg(\n\u001b[1;32m-> 1562\u001b[1;33m             err, \"a Variable name or other graph key that is missing\")\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m       \u001b[1;31m# This is an object-based checkpoint. We'll print a warning and then do\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam not found in checkpoint\n\t [[node save_2/RestoreV2 (defined at <ipython-input-24-360790a6aa8a>:2)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save_2/RestoreV2', defined at:\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-24-360790a6aa8a>\", line 2, in <module>\n    saver = tf.train.Saver()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1102, in __init__\n    self.build()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 795, in _build_internal\n    restore_sequentially, reshape)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 406, in _AddRestoreOps\n    restore_sequentially)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 862, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1466, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:\n\nKey Variable/Adam not found in checkpoint\n\t [[node save_2/RestoreV2 (defined at <ipython-input-24-360790a6aa8a>:2)  = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_2/Const_0_0, save_2/RestoreV2/tensor_names, save_2/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "#saver.restore(sess, tf.train.latest_checkpoint(os.path.join(currentDir,'models')))\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đó, với mỗi batch dữ liệu test, ta sẽ tiến hành test và tính độ chính xác"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDo 3.6: Test mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel\n\t [[node rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read (defined at <ipython-input-17-d1fc5fa17842>:14)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel)]]\n\t [[{{node Mean/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_323_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read', defined at:\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-d1fc5fa17842>\", line 14, in <module>\n    outputs, _ = tf.nn.dynamic_rnn(lstmLayers,data,dtype=tf.float32,initial_state=initial_state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 664, in dynamic_rnn\n    dtype=dtype)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 872, in _dynamic_rnn_loop\n    swap_memory=swap_memory)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3291, in while_loop\n    return_same_structure)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3004, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2939, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3260, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 840, in _time_step\n    (output, new_state) = call_cell()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 826, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 233, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 757, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1486, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1282, in __call__\n    output, new_state = self._cell(inputs, state, scope=scope)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 370, in __call__\n    *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 746, in __call__\n    self.build(input_shapes)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 149, in wrapper\n    output_shape = fn(instance, input_shape)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 716, in build\n    shape=[input_depth + h_depth, 4 * self._num_units])\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 495, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 288, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 609, in add_weight\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 639, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1487, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1237, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 523, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 236, in _rnn_get_variable\n    variable = getter(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 492, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 922, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1488, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3454, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel\n\t [[node rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read (defined at <ipython-input-17-d1fc5fa17842>:14)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel)]]\n\t [[{{node Mean/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_323_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel\n\t [[{{node rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read}} = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel)]]\n\t [[{{node Mean/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_323_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-fd7c4a3b3f34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnextBatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnextBatchLabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetTestBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# TODO 3.6: Tính độ chính xác 'accuracy' trên các test batch và gán vào 'test_acc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnextBatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnextBatchLabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy for this batch:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel\n\t [[node rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read (defined at <ipython-input-17-d1fc5fa17842>:14)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel)]]\n\t [[{{node Mean/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_323_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read', defined at:\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 422, in run_forever\n    self._run_once()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1432, in _run_once\n    handle._run()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-17-d1fc5fa17842>\", line 14, in <module>\n    outputs, _ = tf.nn.dynamic_rnn(lstmLayers,data,dtype=tf.float32,initial_state=initial_state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 664, in dynamic_rnn\n    dtype=dtype)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 872, in _dynamic_rnn_loop\n    swap_memory=swap_memory)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3291, in while_loop\n    return_same_structure)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3004, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2939, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 3260, in <lambda>\n    body = lambda i, lv: (i + 1, orig_body(*lv))\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 840, in _time_step\n    (output, new_state) = call_cell()\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 826, in <lambda>\n    call_cell = lambda: cell(input_t, state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 233, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 757, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1486, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 1282, in __call__\n    output, new_state = self._cell(inputs, state, scope=scope)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 370, in __call__\n    *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 374, in __call__\n    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 746, in __call__\n    self.build(input_shapes)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\tf_utils.py\", line 149, in wrapper\n    output_shape = fn(instance, input_shape)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 716, in build\n    shape=[input_depth + h_depth, 4 * self._num_units])\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 495, in add_variable\n    return self.add_weight(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 288, in add_weight\n    getter=vs.get_variable)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 609, in add_weight\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\base.py\", line 639, in _add_variable_with_custom_getter\n    **kwargs_for_getter)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1487, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1237, in get_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 523, in get_variable\n    return custom_getter(**custom_getter_kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py\", line 236, in _rnn_get_variable\n    variable = getter(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 492, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 922, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1488, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 81, in identity\n    return gen_array_ops.identity(input, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\", line 3454, in identity\n    \"Identity\", input=input, name=name)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\LENOVO LEGION\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel\n\t [[node rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel/read (defined at <ipython-input-17-d1fc5fa17842>:14)  = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel)]]\n\t [[{{node Mean/_53}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_323_Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "# Test on 10 batches\n",
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch()\n",
    "    # TODO 3.6: Tính độ chính xác 'accuracy' trên các test batch và gán vào 'test_acc'\n",
    "    test_acc = sess.run(accuracy,feed_dict={inputs:nextBatch,labels:nextBatchLabels})\n",
    "    print(\"Accuracy for this batch:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do các bộ test được lấy ngẫu nhiên nên độ chính xác trong quá trình này cũng dao động từ 70% đến 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDo 3.7: Viết hàm tổng hợp để dự đoán cảm xúc từ câu tiếng Việt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu cuối cùng này đòi hỏi đòi hỏi các bạn phải vận dụng tư duy tổng hợp để gom tất cả những bước đã thực hiện trước đó thành một quy trình hoàn chỉnh. Các bạn cần viết một hàm hoàn chỉnh với đầu vào là  một câu tiếng Việt cho trước, đầu ra là cho biết câu trên có cảm xúc tích cực hay tiêu cực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wordsList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-abf3aa6050cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# để dự đoán xem câu này có cảm xúc tích cực hay tiêu cực\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Câu này làm khá dài và có tính chất tổng hợp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msentenceIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'món'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msentenceIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'này'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msentenceIndexes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordsList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ăn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wordsList' is not defined"
     ]
    }
   ],
   "source": [
    "input_sentence = 'Món này ăn ngon mê ly luôn. Vị ngọt và thơm quá trời quá đất.'\n",
    "# TODO 3.7 Các bạn vận dụng toàn bộ quy trình đã thực hiện trước đó\n",
    "# để dự đoán xem câu này có cảm xúc tích cực hay tiêu cực\n",
    "# Câu này làm khá dài và có tính chất tổng hợp\n",
    "cleanSen = cleanSentences(input_sentence)\n",
    "split = cleanSen.split()\n",
    "zero_ = np.zeros((1,maxSeqLength),dtype = 'int32')\n",
    "index = 0\n",
    "for _ in split:\n",
    "    try:\n",
    "        zero_[0][index] = wordsList.index(_)\n",
    "    except:\n",
    "        zero_[0][index] = wordsList.index('UNK')\n",
    "    index +=1\n",
    "    if index >= 180:\n",
    "        break\n",
    "\n",
    "inputData = tf.placeholder(tf.int32,[1, maxSeqLength])\n",
    "\n",
    "data  = tf.nn.embedding_lookup(wordVectors, inputData)\n",
    "\n",
    "init_state = lstmLayers.zero_state(1, tf.float32)\n",
    "\n",
    "value,_ = tf.nn.dynamic_rnn(lstmLayers, data, dtype=tf.float32, initial_state=initial_state)\n",
    "\n",
    "last = tf.gather(value, int(value.get_shape()[2]-1))\n",
    "\n",
    "prediction = (tf.matmul(last, weight)+bias)\n",
    "\n",
    "label = tf.argmax(prediction, 1)\n",
    "\n",
    "print(labels[sess.run(label, {inputData: zero_})[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như vậy qua bài tập này, các bạn được ôn lại mô hình Word2Vec và sử dụng mô hình này để biểu diễn cho một văn bản. Sử dụng cách biểu diễn này để đưa vào mô hình RNN với nhiều đơn vị LSTM. Các bạn có thể thử nghiệm trên các cấu hình khác nhau bằng cách thay đổi các hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
